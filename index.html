  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="data:application/x-javascript;base64,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"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">


<h1 class="title">Practical Machine Learning Final</h1>

</div>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>In the course of this assignment, we will use R to create a machine learning algorithm designed to take input from a series of accelerometers and determine in which of 5 ways a lifting exercise is being performed. First, we must load the data into R and examine its basic features:</p>
<pre class="r"><code>library(caret)</code></pre>
<pre><code>## Warning: package 'caret' was built under R version 3.2.5</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## Loading required package: ggplot2</code></pre>
<pre><code>## Warning: package 'ggplot2' was built under R version 3.2.4</code></pre>
<pre class="r"><code>test = read.csv(&quot;pml-testing.csv&quot;)
train = read.csv(&quot;pml-training.csv&quot;)
ncol(train)</code></pre>
<pre><code>## [1] 160</code></pre>
<pre class="r"><code>nrow(train)</code></pre>
<pre><code>## [1] 19622</code></pre>
<pre class="r"><code>nrow(test)</code></pre>
<pre><code>## [1] 20</code></pre>
<p>As expected, our training set is significantly larger than our test set. While it was omitted in the above code for readability purposes, we also call head() on our dataset to take a quick look at the variables. To improve our accuracy, we will remove those with insignificant variance. We will also remove the column X, as it is merely an index and contains no usable information. While we’re at it, we’ll create a version of the test set with the target variable removed:</p>
<pre class="r"><code>cutcol = nearZeroVar(train, saveMetrics = TRUE)
train = train[,cutcol$nzv == FALSE]
test = test[,cutcol$nzv == FALSE]
train$X = NULL
test$X = NULL
test$user_name = NULL
train$user_name = NULL</code></pre>
<p>However, this still leaves us with a number of NAs in our data, which is problematic for many analyses. Examination of the dataset reveals that NAs are heavily clustered in a few columns and can be safely removed:</p>
<pre class="r"><code>train = train[,colMeans(is.na(train)) == 0]
test = test[,colMeans(is.na(test)) == 0]</code></pre>
<p>In order to avoid overfitting to the test set, we will set up cross validation using a 70-30 split of our training set:</p>
<pre class="r"><code>set.seed = 1
split = createDataPartition(train$classe, p = 0.7, list = FALSE)
crosstrain = train[split,]
crossval = train[-split,]</code></pre>
<p>Finally, we can begin to construct our model. For this assignment, we will be using a random forest to interpret the data. We use the training portion of our partitioned dataset, check the results using the cross validation portion, and examine the accuracy using confusionMatrix:</p>
<pre class="r"><code>library(randomForest)</code></pre>
<pre><code>## Warning: package 'randomForest' was built under R version 3.2.5</code></pre>
<pre><code>## randomForest 4.6-12</code></pre>
<pre><code>## Type rfNews() to see new features/changes/bug fixes.</code></pre>
<pre><code>## 
## Attaching package: 'randomForest'</code></pre>
<pre><code>## The following object is masked from 'package:ggplot2':
## 
##     margin</code></pre>
<pre class="r"><code>library(rpart)</code></pre>
<pre><code>## Warning: package 'rpart' was built under R version 3.2.5</code></pre>
<pre class="r"><code>forest = randomForest(classe ~ ., data=crosstrain, method=&quot;class&quot;)
crosspred = predict(forest, crossval, type = &quot;class&quot;)
confusionMatrix(crosspred, crossval$classe)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1674    2    0    0    0
##          B    0 1137    1    0    0
##          C    0    0 1024    1    0
##          D    0    0    1  963    0
##          E    0    0    0    0 1082
## 
## Overall Statistics
##                                          
##                Accuracy : 0.9992         
##                  95% CI : (0.998, 0.9997)
##     No Information Rate : 0.2845         
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16      
##                                          
##                   Kappa : 0.9989         
##  Mcnemar's Test P-Value : NA             
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            1.0000   0.9982   0.9981   0.9990   1.0000
## Specificity            0.9995   0.9998   0.9998   0.9998   1.0000
## Pos Pred Value         0.9988   0.9991   0.9990   0.9990   1.0000
## Neg Pred Value         1.0000   0.9996   0.9996   0.9998   1.0000
## Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
## Detection Rate         0.2845   0.1932   0.1740   0.1636   0.1839
## Detection Prevalence   0.2848   0.1934   0.1742   0.1638   0.1839
## Balanced Accuracy      0.9998   0.9990   0.9989   0.9994   1.0000</code></pre>
<p>Our accuracy is above 99%, which is very good! Note specifically that this is a cross validation test and not merely application to the existing data, so we also anticipate an out-of-sample error rate of less than 1%. Since our cross validation has yielded such positive results, we will use a random forest to generate our final outcome using the full training set and apply it to the test set. Initial attempts produced errors relating to predictors not matching; these are addressed by quickly cleaning up factor levels before creating the full forest.</p>
<pre class="r"><code>common = intersect(names(train), names(test)) 
for (p in common) { 
  if (class(train[[p]]) == &quot;factor&quot;) { 
    levels(test[[p]]) = levels(train[[p]]) 
  } 
}


finalForest = randomForest(classe ~ ., data=train, method=&quot;class&quot;)
test$problem_id = as.factor(test$problem_id)
predict(forest, newdata = test, type = &quot;class&quot;)</code></pre>
<pre><code>##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 
##  B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B 
## Levels: A B C D E</code></pre>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>